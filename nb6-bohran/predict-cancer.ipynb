{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Breast Cancer\n",
    "In this exercise, we'll explore a valuable application of Machine Learning: predicting whether or not tumors are **benign** or **malignant**. We'll use the (real) [Wisconsin Breast Cancer dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names), that is included in the `sklearn` library. Through this exercise, you'll learn the steps associated with implementing a Machine Learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.71190</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.570</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.24160</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.690</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.290</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.14420</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.37840</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.710</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.16540</td>\n",
       "      <td>0.36820</td>\n",
       "      <td>0.26780</td>\n",
       "      <td>0.15560</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.000</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.17030</td>\n",
       "      <td>0.54010</td>\n",
       "      <td>0.53900</td>\n",
       "      <td>0.20600</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.085430</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.10500</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.14590</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.780</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.066060</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>...</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.13960</td>\n",
       "      <td>0.56090</td>\n",
       "      <td>0.39650</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.170</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>...</td>\n",
       "      <td>29.94</td>\n",
       "      <td>151.70</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0.10370</td>\n",
       "      <td>0.39030</td>\n",
       "      <td>0.36390</td>\n",
       "      <td>0.17670</td>\n",
       "      <td>0.3176</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.850</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.099380</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>...</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.19240</td>\n",
       "      <td>0.23220</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.730</td>\n",
       "      <td>22.61</td>\n",
       "      <td>93.60</td>\n",
       "      <td>578.3</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.22930</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.080250</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.07682</td>\n",
       "      <td>...</td>\n",
       "      <td>32.01</td>\n",
       "      <td>108.80</td>\n",
       "      <td>697.7</td>\n",
       "      <td>0.16510</td>\n",
       "      <td>0.77250</td>\n",
       "      <td>0.69430</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.3596</td>\n",
       "      <td>0.14310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.540</td>\n",
       "      <td>27.54</td>\n",
       "      <td>96.73</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.15950</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.073640</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.07077</td>\n",
       "      <td>...</td>\n",
       "      <td>37.13</td>\n",
       "      <td>124.10</td>\n",
       "      <td>943.2</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.65770</td>\n",
       "      <td>0.70260</td>\n",
       "      <td>0.17120</td>\n",
       "      <td>0.4218</td>\n",
       "      <td>0.13410</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.680</td>\n",
       "      <td>20.13</td>\n",
       "      <td>94.74</td>\n",
       "      <td>684.5</td>\n",
       "      <td>0.09867</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>0.073950</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>0.1586</td>\n",
       "      <td>0.05922</td>\n",
       "      <td>...</td>\n",
       "      <td>30.88</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.14640</td>\n",
       "      <td>0.18710</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.16090</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.08216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.130</td>\n",
       "      <td>20.68</td>\n",
       "      <td>108.10</td>\n",
       "      <td>798.8</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.20220</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.07356</td>\n",
       "      <td>...</td>\n",
       "      <td>31.48</td>\n",
       "      <td>136.80</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.17890</td>\n",
       "      <td>0.42330</td>\n",
       "      <td>0.47840</td>\n",
       "      <td>0.20730</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.11420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.810</td>\n",
       "      <td>22.15</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.09831</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.094980</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05395</td>\n",
       "      <td>...</td>\n",
       "      <td>30.88</td>\n",
       "      <td>186.80</td>\n",
       "      <td>2398.0</td>\n",
       "      <td>0.15120</td>\n",
       "      <td>0.31500</td>\n",
       "      <td>0.53720</td>\n",
       "      <td>0.23880</td>\n",
       "      <td>0.2768</td>\n",
       "      <td>0.07615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.05766</td>\n",
       "      <td>...</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.045680</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>...</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>...</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.340</td>\n",
       "      <td>14.26</td>\n",
       "      <td>102.50</td>\n",
       "      <td>704.4</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.097560</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.07032</td>\n",
       "      <td>...</td>\n",
       "      <td>19.08</td>\n",
       "      <td>125.10</td>\n",
       "      <td>980.9</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.59540</td>\n",
       "      <td>0.63050</td>\n",
       "      <td>0.23930</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.09946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.160</td>\n",
       "      <td>23.04</td>\n",
       "      <td>137.20</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.09428</td>\n",
       "      <td>0.10220</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.05278</td>\n",
       "      <td>...</td>\n",
       "      <td>35.59</td>\n",
       "      <td>188.00</td>\n",
       "      <td>2615.0</td>\n",
       "      <td>0.14010</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.31550</td>\n",
       "      <td>0.20090</td>\n",
       "      <td>0.2822</td>\n",
       "      <td>0.07526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.650</td>\n",
       "      <td>21.38</td>\n",
       "      <td>110.00</td>\n",
       "      <td>904.6</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.06330</td>\n",
       "      <td>...</td>\n",
       "      <td>31.56</td>\n",
       "      <td>177.00</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>0.18050</td>\n",
       "      <td>0.35780</td>\n",
       "      <td>0.46950</td>\n",
       "      <td>0.20950</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.09564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.140</td>\n",
       "      <td>16.40</td>\n",
       "      <td>116.00</td>\n",
       "      <td>912.7</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.22760</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.07413</td>\n",
       "      <td>...</td>\n",
       "      <td>21.40</td>\n",
       "      <td>152.40</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>0.15450</td>\n",
       "      <td>0.39490</td>\n",
       "      <td>0.38530</td>\n",
       "      <td>0.25500</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.10590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.580</td>\n",
       "      <td>21.53</td>\n",
       "      <td>97.41</td>\n",
       "      <td>644.8</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.18680</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.087830</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.06924</td>\n",
       "      <td>...</td>\n",
       "      <td>33.21</td>\n",
       "      <td>122.40</td>\n",
       "      <td>896.9</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.66430</td>\n",
       "      <td>0.55390</td>\n",
       "      <td>0.27010</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.610</td>\n",
       "      <td>20.25</td>\n",
       "      <td>122.10</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>0.09440</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.077310</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.05699</td>\n",
       "      <td>...</td>\n",
       "      <td>27.26</td>\n",
       "      <td>139.90</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>0.13380</td>\n",
       "      <td>0.21170</td>\n",
       "      <td>0.34460</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.07421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15.300</td>\n",
       "      <td>25.27</td>\n",
       "      <td>102.40</td>\n",
       "      <td>732.4</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.06540</td>\n",
       "      <td>...</td>\n",
       "      <td>36.71</td>\n",
       "      <td>149.30</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.61100</td>\n",
       "      <td>0.63350</td>\n",
       "      <td>0.20240</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.09876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.570</td>\n",
       "      <td>15.05</td>\n",
       "      <td>115.00</td>\n",
       "      <td>955.1</td>\n",
       "      <td>0.09847</td>\n",
       "      <td>0.11570</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>0.079530</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.06149</td>\n",
       "      <td>...</td>\n",
       "      <td>19.52</td>\n",
       "      <td>134.90</td>\n",
       "      <td>1227.0</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.28120</td>\n",
       "      <td>0.24890</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.07919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>7.691</td>\n",
       "      <td>25.44</td>\n",
       "      <td>48.34</td>\n",
       "      <td>170.4</td>\n",
       "      <td>0.08668</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.092520</td>\n",
       "      <td>0.013640</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>0.07751</td>\n",
       "      <td>...</td>\n",
       "      <td>31.89</td>\n",
       "      <td>54.49</td>\n",
       "      <td>223.6</td>\n",
       "      <td>0.15960</td>\n",
       "      <td>0.30640</td>\n",
       "      <td>0.33930</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.2790</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>11.540</td>\n",
       "      <td>14.44</td>\n",
       "      <td>74.65</td>\n",
       "      <td>402.9</td>\n",
       "      <td>0.09984</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.067370</td>\n",
       "      <td>0.025940</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.06782</td>\n",
       "      <td>...</td>\n",
       "      <td>19.68</td>\n",
       "      <td>78.78</td>\n",
       "      <td>457.8</td>\n",
       "      <td>0.13450</td>\n",
       "      <td>0.21180</td>\n",
       "      <td>0.17970</td>\n",
       "      <td>0.06918</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.08134</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>14.470</td>\n",
       "      <td>24.99</td>\n",
       "      <td>95.81</td>\n",
       "      <td>656.4</td>\n",
       "      <td>0.08837</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.06341</td>\n",
       "      <td>...</td>\n",
       "      <td>31.73</td>\n",
       "      <td>113.50</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.13400</td>\n",
       "      <td>0.42020</td>\n",
       "      <td>0.40400</td>\n",
       "      <td>0.12050</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>14.740</td>\n",
       "      <td>25.42</td>\n",
       "      <td>94.70</td>\n",
       "      <td>668.6</td>\n",
       "      <td>0.08275</td>\n",
       "      <td>0.07214</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.05680</td>\n",
       "      <td>...</td>\n",
       "      <td>32.29</td>\n",
       "      <td>107.40</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.13760</td>\n",
       "      <td>0.16110</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.2722</td>\n",
       "      <td>0.06956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>13.210</td>\n",
       "      <td>28.06</td>\n",
       "      <td>84.88</td>\n",
       "      <td>538.4</td>\n",
       "      <td>0.08671</td>\n",
       "      <td>0.06877</td>\n",
       "      <td>0.029870</td>\n",
       "      <td>0.032750</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.05781</td>\n",
       "      <td>...</td>\n",
       "      <td>37.17</td>\n",
       "      <td>92.48</td>\n",
       "      <td>629.6</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.13810</td>\n",
       "      <td>0.10620</td>\n",
       "      <td>0.07958</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.06443</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>13.870</td>\n",
       "      <td>20.70</td>\n",
       "      <td>89.77</td>\n",
       "      <td>584.8</td>\n",
       "      <td>0.09578</td>\n",
       "      <td>0.10180</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.023690</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.06688</td>\n",
       "      <td>...</td>\n",
       "      <td>24.75</td>\n",
       "      <td>99.17</td>\n",
       "      <td>688.6</td>\n",
       "      <td>0.12640</td>\n",
       "      <td>0.20370</td>\n",
       "      <td>0.13770</td>\n",
       "      <td>0.06845</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>0.08492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>13.620</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.05801</td>\n",
       "      <td>...</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.07174</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.06953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>10.320</td>\n",
       "      <td>16.35</td>\n",
       "      <td>65.31</td>\n",
       "      <td>324.9</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.04994</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.06201</td>\n",
       "      <td>...</td>\n",
       "      <td>21.77</td>\n",
       "      <td>71.12</td>\n",
       "      <td>384.9</td>\n",
       "      <td>0.12850</td>\n",
       "      <td>0.08842</td>\n",
       "      <td>0.04384</td>\n",
       "      <td>0.02381</td>\n",
       "      <td>0.2681</td>\n",
       "      <td>0.07399</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>10.260</td>\n",
       "      <td>16.58</td>\n",
       "      <td>65.85</td>\n",
       "      <td>320.8</td>\n",
       "      <td>0.08877</td>\n",
       "      <td>0.08066</td>\n",
       "      <td>0.043580</td>\n",
       "      <td>0.024380</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06714</td>\n",
       "      <td>...</td>\n",
       "      <td>22.04</td>\n",
       "      <td>71.08</td>\n",
       "      <td>357.4</td>\n",
       "      <td>0.14610</td>\n",
       "      <td>0.22460</td>\n",
       "      <td>0.17830</td>\n",
       "      <td>0.08333</td>\n",
       "      <td>0.2691</td>\n",
       "      <td>0.09479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>9.683</td>\n",
       "      <td>19.34</td>\n",
       "      <td>61.05</td>\n",
       "      <td>285.7</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>0.05030</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.06235</td>\n",
       "      <td>...</td>\n",
       "      <td>25.59</td>\n",
       "      <td>69.10</td>\n",
       "      <td>364.2</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.09546</td>\n",
       "      <td>0.09350</td>\n",
       "      <td>0.03846</td>\n",
       "      <td>0.2552</td>\n",
       "      <td>0.07920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>10.820</td>\n",
       "      <td>24.21</td>\n",
       "      <td>68.89</td>\n",
       "      <td>361.6</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.06602</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>0.06328</td>\n",
       "      <td>...</td>\n",
       "      <td>31.45</td>\n",
       "      <td>83.90</td>\n",
       "      <td>505.6</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.16330</td>\n",
       "      <td>0.06194</td>\n",
       "      <td>0.03264</td>\n",
       "      <td>0.3059</td>\n",
       "      <td>0.07626</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>10.860</td>\n",
       "      <td>21.48</td>\n",
       "      <td>68.51</td>\n",
       "      <td>360.5</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.04227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1661</td>\n",
       "      <td>0.05948</td>\n",
       "      <td>...</td>\n",
       "      <td>24.77</td>\n",
       "      <td>74.08</td>\n",
       "      <td>412.3</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06592</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>11.130</td>\n",
       "      <td>22.44</td>\n",
       "      <td>71.49</td>\n",
       "      <td>378.4</td>\n",
       "      <td>0.09566</td>\n",
       "      <td>0.08194</td>\n",
       "      <td>0.048240</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.06552</td>\n",
       "      <td>...</td>\n",
       "      <td>28.26</td>\n",
       "      <td>77.80</td>\n",
       "      <td>436.6</td>\n",
       "      <td>0.10870</td>\n",
       "      <td>0.17820</td>\n",
       "      <td>0.15640</td>\n",
       "      <td>0.06413</td>\n",
       "      <td>0.3169</td>\n",
       "      <td>0.08032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>12.770</td>\n",
       "      <td>29.43</td>\n",
       "      <td>81.35</td>\n",
       "      <td>507.9</td>\n",
       "      <td>0.08276</td>\n",
       "      <td>0.04234</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.05637</td>\n",
       "      <td>...</td>\n",
       "      <td>36.00</td>\n",
       "      <td>88.10</td>\n",
       "      <td>594.7</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.08653</td>\n",
       "      <td>0.06498</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>0.06484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>9.333</td>\n",
       "      <td>21.94</td>\n",
       "      <td>59.01</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.09240</td>\n",
       "      <td>0.05605</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.012820</td>\n",
       "      <td>0.1692</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>...</td>\n",
       "      <td>25.05</td>\n",
       "      <td>62.86</td>\n",
       "      <td>295.8</td>\n",
       "      <td>0.11030</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.07993</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.07393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>12.880</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.061950</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>...</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.12270</td>\n",
       "      <td>0.16200</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.07242</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>10.290</td>\n",
       "      <td>27.61</td>\n",
       "      <td>65.67</td>\n",
       "      <td>321.4</td>\n",
       "      <td>0.09030</td>\n",
       "      <td>0.07658</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.06127</td>\n",
       "      <td>...</td>\n",
       "      <td>34.91</td>\n",
       "      <td>69.57</td>\n",
       "      <td>357.6</td>\n",
       "      <td>0.13840</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.09127</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.08283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>10.160</td>\n",
       "      <td>19.59</td>\n",
       "      <td>64.73</td>\n",
       "      <td>311.7</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.07504</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.06331</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>67.88</td>\n",
       "      <td>347.3</td>\n",
       "      <td>0.12650</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.01005</td>\n",
       "      <td>0.02232</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.06742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>9.423</td>\n",
       "      <td>27.88</td>\n",
       "      <td>59.26</td>\n",
       "      <td>271.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.04971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>...</td>\n",
       "      <td>34.24</td>\n",
       "      <td>66.50</td>\n",
       "      <td>330.6</td>\n",
       "      <td>0.10730</td>\n",
       "      <td>0.07158</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.06969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>14.590</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.06147</td>\n",
       "      <td>...</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.36620</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.08004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.111200</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>0.06570</td>\n",
       "      <td>...</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>14.050</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>0.06171</td>\n",
       "      <td>...</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>11.200</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.05502</td>\n",
       "      <td>...</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.220</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>...</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>20.920</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>0.06879</td>\n",
       "      <td>...</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.41860</td>\n",
       "      <td>0.65990</td>\n",
       "      <td>0.25420</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.09873</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.560</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.138900</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.41070</td>\n",
       "      <td>0.22160</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.130</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.144000</td>\n",
       "      <td>0.097910</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>0.16280</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.600</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>0.053020</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.34030</td>\n",
       "      <td>0.14180</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.600</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.93870</td>\n",
       "      <td>0.26500</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.760</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0         17.990         10.38          122.80     1001.0          0.11840   \n",
       "1         20.570         17.77          132.90     1326.0          0.08474   \n",
       "2         19.690         21.25          130.00     1203.0          0.10960   \n",
       "3         11.420         20.38           77.58      386.1          0.14250   \n",
       "4         20.290         14.34          135.10     1297.0          0.10030   \n",
       "5         12.450         15.70           82.57      477.1          0.12780   \n",
       "6         18.250         19.98          119.60     1040.0          0.09463   \n",
       "7         13.710         20.83           90.20      577.9          0.11890   \n",
       "8         13.000         21.82           87.50      519.8          0.12730   \n",
       "9         12.460         24.04           83.97      475.9          0.11860   \n",
       "10        16.020         23.24          102.70      797.8          0.08206   \n",
       "11        15.780         17.89          103.60      781.0          0.09710   \n",
       "12        19.170         24.80          132.40     1123.0          0.09740   \n",
       "13        15.850         23.95          103.70      782.7          0.08401   \n",
       "14        13.730         22.61           93.60      578.3          0.11310   \n",
       "15        14.540         27.54           96.73      658.8          0.11390   \n",
       "16        14.680         20.13           94.74      684.5          0.09867   \n",
       "17        16.130         20.68          108.10      798.8          0.11700   \n",
       "18        19.810         22.15          130.00     1260.0          0.09831   \n",
       "19        13.540         14.36           87.46      566.3          0.09779   \n",
       "20        13.080         15.71           85.63      520.0          0.10750   \n",
       "21         9.504         12.44           60.34      273.9          0.10240   \n",
       "22        15.340         14.26          102.50      704.4          0.10730   \n",
       "23        21.160         23.04          137.20     1404.0          0.09428   \n",
       "24        16.650         21.38          110.00      904.6          0.11210   \n",
       "25        17.140         16.40          116.00      912.7          0.11860   \n",
       "26        14.580         21.53           97.41      644.8          0.10540   \n",
       "27        18.610         20.25          122.10     1094.0          0.09440   \n",
       "28        15.300         25.27          102.40      732.4          0.10820   \n",
       "29        17.570         15.05          115.00      955.1          0.09847   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "539        7.691         25.44           48.34      170.4          0.08668   \n",
       "540       11.540         14.44           74.65      402.9          0.09984   \n",
       "541       14.470         24.99           95.81      656.4          0.08837   \n",
       "542       14.740         25.42           94.70      668.6          0.08275   \n",
       "543       13.210         28.06           84.88      538.4          0.08671   \n",
       "544       13.870         20.70           89.77      584.8          0.09578   \n",
       "545       13.620         23.23           87.19      573.2          0.09246   \n",
       "546       10.320         16.35           65.31      324.9          0.09434   \n",
       "547       10.260         16.58           65.85      320.8          0.08877   \n",
       "548        9.683         19.34           61.05      285.7          0.08491   \n",
       "549       10.820         24.21           68.89      361.6          0.08192   \n",
       "550       10.860         21.48           68.51      360.5          0.07431   \n",
       "551       11.130         22.44           71.49      378.4          0.09566   \n",
       "552       12.770         29.43           81.35      507.9          0.08276   \n",
       "553        9.333         21.94           59.01      264.0          0.09240   \n",
       "554       12.880         28.92           82.50      514.3          0.08123   \n",
       "555       10.290         27.61           65.67      321.4          0.09030   \n",
       "556       10.160         19.59           64.73      311.7          0.10030   \n",
       "557        9.423         27.88           59.26      271.3          0.08123   \n",
       "558       14.590         22.68           96.39      657.1          0.08473   \n",
       "559       11.510         23.93           74.52      403.5          0.09261   \n",
       "560       14.050         27.15           91.38      600.4          0.09929   \n",
       "561       11.200         29.37           70.67      386.0          0.07449   \n",
       "562       15.220         30.62          103.40      716.9          0.10480   \n",
       "563       20.920         25.09          143.00     1347.0          0.10990   \n",
       "564       21.560         22.39          142.00     1479.0          0.11100   \n",
       "565       20.130         28.25          131.20     1261.0          0.09780   \n",
       "566       16.600         28.08          108.30      858.1          0.08455   \n",
       "567       20.600         29.33          140.10     1265.0          0.11780   \n",
       "568        7.760         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760        0.300100             0.147100         0.2419   \n",
       "1             0.07864        0.086900             0.070170         0.1812   \n",
       "2             0.15990        0.197400             0.127900         0.2069   \n",
       "3             0.28390        0.241400             0.105200         0.2597   \n",
       "4             0.13280        0.198000             0.104300         0.1809   \n",
       "5             0.17000        0.157800             0.080890         0.2087   \n",
       "6             0.10900        0.112700             0.074000         0.1794   \n",
       "7             0.16450        0.093660             0.059850         0.2196   \n",
       "8             0.19320        0.185900             0.093530         0.2350   \n",
       "9             0.23960        0.227300             0.085430         0.2030   \n",
       "10            0.06669        0.032990             0.033230         0.1528   \n",
       "11            0.12920        0.099540             0.066060         0.1842   \n",
       "12            0.24580        0.206500             0.111800         0.2397   \n",
       "13            0.10020        0.099380             0.053640         0.1847   \n",
       "14            0.22930        0.212800             0.080250         0.2069   \n",
       "15            0.15950        0.163900             0.073640         0.2303   \n",
       "16            0.07200        0.073950             0.052590         0.1586   \n",
       "17            0.20220        0.172200             0.102800         0.2164   \n",
       "18            0.10270        0.147900             0.094980         0.1582   \n",
       "19            0.08129        0.066640             0.047810         0.1885   \n",
       "20            0.12700        0.045680             0.031100         0.1967   \n",
       "21            0.06492        0.029560             0.020760         0.1815   \n",
       "22            0.21350        0.207700             0.097560         0.2521   \n",
       "23            0.10220        0.109700             0.086320         0.1769   \n",
       "24            0.14570        0.152500             0.091700         0.1995   \n",
       "25            0.22760        0.222900             0.140100         0.3040   \n",
       "26            0.18680        0.142500             0.087830         0.2252   \n",
       "27            0.10660        0.149000             0.077310         0.1697   \n",
       "28            0.16970        0.168300             0.087510         0.1926   \n",
       "29            0.11570        0.098750             0.079530         0.1739   \n",
       "..                ...             ...                  ...            ...   \n",
       "539           0.11990        0.092520             0.013640         0.2037   \n",
       "540           0.11200        0.067370             0.025940         0.1818   \n",
       "541           0.12300        0.100900             0.038900         0.1872   \n",
       "542           0.07214        0.041050             0.030270         0.1840   \n",
       "543           0.06877        0.029870             0.032750         0.1628   \n",
       "544           0.10180        0.036880             0.023690         0.1620   \n",
       "545           0.06747        0.029740             0.024430         0.1664   \n",
       "546           0.04994        0.010120             0.005495         0.1885   \n",
       "547           0.08066        0.043580             0.024380         0.1669   \n",
       "548           0.05030        0.023370             0.009615         0.1580   \n",
       "549           0.06602        0.015480             0.008160         0.1976   \n",
       "550           0.04227        0.000000             0.000000         0.1661   \n",
       "551           0.08194        0.048240             0.022570         0.2030   \n",
       "552           0.04234        0.019970             0.014990         0.1539   \n",
       "553           0.05605        0.039960             0.012820         0.1692   \n",
       "554           0.05824        0.061950             0.023430         0.1566   \n",
       "555           0.07658        0.059990             0.027380         0.1593   \n",
       "556           0.07504        0.005025             0.011160         0.1791   \n",
       "557           0.04971        0.000000             0.000000         0.1742   \n",
       "558           0.13300        0.102900             0.037360         0.1454   \n",
       "559           0.10210        0.111200             0.041050         0.1388   \n",
       "560           0.11260        0.044620             0.043040         0.1537   \n",
       "561           0.03558        0.000000             0.000000         0.1060   \n",
       "562           0.20870        0.255000             0.094290         0.2128   \n",
       "563           0.22360        0.317400             0.147400         0.2149   \n",
       "564           0.11590        0.243900             0.138900         0.1726   \n",
       "565           0.10340        0.144000             0.097910         0.1752   \n",
       "566           0.10230        0.092510             0.053020         0.1590   \n",
       "567           0.27700        0.351400             0.152000         0.2397   \n",
       "568           0.04362        0.000000             0.000000         0.1587   \n",
       "\n",
       "     mean fractal dimension   ...     worst texture  worst perimeter  \\\n",
       "0                   0.07871   ...             17.33           184.60   \n",
       "1                   0.05667   ...             23.41           158.80   \n",
       "2                   0.05999   ...             25.53           152.50   \n",
       "3                   0.09744   ...             26.50            98.87   \n",
       "4                   0.05883   ...             16.67           152.20   \n",
       "5                   0.07613   ...             23.75           103.40   \n",
       "6                   0.05742   ...             27.66           153.20   \n",
       "7                   0.07451   ...             28.14           110.60   \n",
       "8                   0.07389   ...             30.73           106.20   \n",
       "9                   0.08243   ...             40.68            97.65   \n",
       "10                  0.05697   ...             33.88           123.80   \n",
       "11                  0.06082   ...             27.28           136.50   \n",
       "12                  0.07800   ...             29.94           151.70   \n",
       "13                  0.05338   ...             27.66           112.00   \n",
       "14                  0.07682   ...             32.01           108.80   \n",
       "15                  0.07077   ...             37.13           124.10   \n",
       "16                  0.05922   ...             30.88           123.40   \n",
       "17                  0.07356   ...             31.48           136.80   \n",
       "18                  0.05395   ...             30.88           186.80   \n",
       "19                  0.05766   ...             19.26            99.70   \n",
       "20                  0.06811   ...             20.49            96.09   \n",
       "21                  0.06905   ...             15.66            65.13   \n",
       "22                  0.07032   ...             19.08           125.10   \n",
       "23                  0.05278   ...             35.59           188.00   \n",
       "24                  0.06330   ...             31.56           177.00   \n",
       "25                  0.07413   ...             21.40           152.40   \n",
       "26                  0.06924   ...             33.21           122.40   \n",
       "27                  0.05699   ...             27.26           139.90   \n",
       "28                  0.06540   ...             36.71           149.30   \n",
       "29                  0.06149   ...             19.52           134.90   \n",
       "..                      ...   ...               ...              ...   \n",
       "539                 0.07751   ...             31.89            54.49   \n",
       "540                 0.06782   ...             19.68            78.78   \n",
       "541                 0.06341   ...             31.73           113.50   \n",
       "542                 0.05680   ...             32.29           107.40   \n",
       "543                 0.05781   ...             37.17            92.48   \n",
       "544                 0.06688   ...             24.75            99.17   \n",
       "545                 0.05801   ...             29.09            97.58   \n",
       "546                 0.06201   ...             21.77            71.12   \n",
       "547                 0.06714   ...             22.04            71.08   \n",
       "548                 0.06235   ...             25.59            69.10   \n",
       "549                 0.06328   ...             31.45            83.90   \n",
       "550                 0.05948   ...             24.77            74.08   \n",
       "551                 0.06552   ...             28.26            77.80   \n",
       "552                 0.05637   ...             36.00            88.10   \n",
       "553                 0.06576   ...             25.05            62.86   \n",
       "554                 0.05708   ...             35.74            88.84   \n",
       "555                 0.06127   ...             34.91            69.57   \n",
       "556                 0.06331   ...             22.88            67.88   \n",
       "557                 0.06059   ...             34.24            66.50   \n",
       "558                 0.06147   ...             27.27           105.90   \n",
       "559                 0.06570   ...             37.16            82.28   \n",
       "560                 0.06171   ...             33.17           100.20   \n",
       "561                 0.05502   ...             38.30            75.19   \n",
       "562                 0.07152   ...             42.79           128.70   \n",
       "563                 0.06879   ...             29.41           179.10   \n",
       "564                 0.05623   ...             26.40           166.10   \n",
       "565                 0.05533   ...             38.25           155.00   \n",
       "566                 0.05648   ...             34.12           126.70   \n",
       "567                 0.07016   ...             39.42           184.60   \n",
       "568                 0.05884   ...             30.37            59.16   \n",
       "\n",
       "     worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0        2019.0           0.16220            0.66560          0.71190   \n",
       "1        1956.0           0.12380            0.18660          0.24160   \n",
       "2        1709.0           0.14440            0.42450          0.45040   \n",
       "3         567.7           0.20980            0.86630          0.68690   \n",
       "4        1575.0           0.13740            0.20500          0.40000   \n",
       "5         741.6           0.17910            0.52490          0.53550   \n",
       "6        1606.0           0.14420            0.25760          0.37840   \n",
       "7         897.0           0.16540            0.36820          0.26780   \n",
       "8         739.3           0.17030            0.54010          0.53900   \n",
       "9         711.4           0.18530            1.05800          1.10500   \n",
       "10       1150.0           0.11810            0.15510          0.14590   \n",
       "11       1299.0           0.13960            0.56090          0.39650   \n",
       "12       1332.0           0.10370            0.39030          0.36390   \n",
       "13        876.5           0.11310            0.19240          0.23220   \n",
       "14        697.7           0.16510            0.77250          0.69430   \n",
       "15        943.2           0.16780            0.65770          0.70260   \n",
       "16       1138.0           0.14640            0.18710          0.29140   \n",
       "17       1315.0           0.17890            0.42330          0.47840   \n",
       "18       2398.0           0.15120            0.31500          0.53720   \n",
       "19        711.2           0.14400            0.17730          0.23900   \n",
       "20        630.5           0.13120            0.27760          0.18900   \n",
       "21        314.9           0.13240            0.11480          0.08867   \n",
       "22        980.9           0.13900            0.59540          0.63050   \n",
       "23       2615.0           0.14010            0.26000          0.31550   \n",
       "24       2215.0           0.18050            0.35780          0.46950   \n",
       "25       1461.0           0.15450            0.39490          0.38530   \n",
       "26        896.9           0.15250            0.66430          0.55390   \n",
       "27       1403.0           0.13380            0.21170          0.34460   \n",
       "28       1269.0           0.16410            0.61100          0.63350   \n",
       "29       1227.0           0.12550            0.28120          0.24890   \n",
       "..          ...               ...                ...              ...   \n",
       "539       223.6           0.15960            0.30640          0.33930   \n",
       "540       457.8           0.13450            0.21180          0.17970   \n",
       "541       808.9           0.13400            0.42020          0.40400   \n",
       "542       826.4           0.10600            0.13760          0.16110   \n",
       "543       629.6           0.10720            0.13810          0.10620   \n",
       "544       688.6           0.12640            0.20370          0.13770   \n",
       "545       729.8           0.12160            0.15170          0.10490   \n",
       "546       384.9           0.12850            0.08842          0.04384   \n",
       "547       357.4           0.14610            0.22460          0.17830   \n",
       "548       364.2           0.11990            0.09546          0.09350   \n",
       "549       505.6           0.12040            0.16330          0.06194   \n",
       "550       412.3           0.10010            0.07348          0.00000   \n",
       "551       436.6           0.10870            0.17820          0.15640   \n",
       "552       594.7           0.12340            0.10640          0.08653   \n",
       "553       295.8           0.11030            0.08298          0.07993   \n",
       "554       595.7           0.12270            0.16200          0.24390   \n",
       "555       357.6           0.13840            0.17100          0.20000   \n",
       "556       347.3           0.12650            0.12000          0.01005   \n",
       "557       330.6           0.10730            0.07158          0.00000   \n",
       "558       733.5           0.10260            0.31710          0.36620   \n",
       "559       474.2           0.12980            0.25170          0.36300   \n",
       "560       706.7           0.12410            0.22640          0.13260   \n",
       "561       439.6           0.09267            0.05494          0.00000   \n",
       "562       915.0           0.14170            0.79170          1.17000   \n",
       "563      1819.0           0.14070            0.41860          0.65990   \n",
       "564      2027.0           0.14100            0.21130          0.41070   \n",
       "565      1731.0           0.11660            0.19220          0.32150   \n",
       "566      1124.0           0.11390            0.30940          0.34030   \n",
       "567      1821.0           0.16500            0.86810          0.93870   \n",
       "568       268.6           0.08996            0.06444          0.00000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  outcome  \n",
       "0                 0.26540          0.4601                  0.11890        0  \n",
       "1                 0.18600          0.2750                  0.08902        0  \n",
       "2                 0.24300          0.3613                  0.08758        0  \n",
       "3                 0.25750          0.6638                  0.17300        0  \n",
       "4                 0.16250          0.2364                  0.07678        0  \n",
       "5                 0.17410          0.3985                  0.12440        0  \n",
       "6                 0.19320          0.3063                  0.08368        0  \n",
       "7                 0.15560          0.3196                  0.11510        0  \n",
       "8                 0.20600          0.4378                  0.10720        0  \n",
       "9                 0.22100          0.4366                  0.20750        0  \n",
       "10                0.09975          0.2948                  0.08452        0  \n",
       "11                0.18100          0.3792                  0.10480        0  \n",
       "12                0.17670          0.3176                  0.10230        0  \n",
       "13                0.11190          0.2809                  0.06287        0  \n",
       "14                0.22080          0.3596                  0.14310        0  \n",
       "15                0.17120          0.4218                  0.13410        0  \n",
       "16                0.16090          0.3029                  0.08216        0  \n",
       "17                0.20730          0.3706                  0.11420        0  \n",
       "18                0.23880          0.2768                  0.07615        0  \n",
       "19                0.12880          0.2977                  0.07259        1  \n",
       "20                0.07283          0.3184                  0.08183        1  \n",
       "21                0.06227          0.2450                  0.07773        1  \n",
       "22                0.23930          0.4667                  0.09946        0  \n",
       "23                0.20090          0.2822                  0.07526        0  \n",
       "24                0.20950          0.3613                  0.09564        0  \n",
       "25                0.25500          0.4066                  0.10590        0  \n",
       "26                0.27010          0.4264                  0.12750        0  \n",
       "27                0.14900          0.2341                  0.07421        0  \n",
       "28                0.20240          0.4027                  0.09876        0  \n",
       "29                0.14560          0.2756                  0.07919        0  \n",
       "..                    ...             ...                      ...      ...  \n",
       "539               0.05000          0.2790                  0.10660        1  \n",
       "540               0.06918          0.2329                  0.08134        1  \n",
       "541               0.12050          0.3187                  0.10230        1  \n",
       "542               0.10950          0.2722                  0.06956        1  \n",
       "543               0.07958          0.2473                  0.06443        1  \n",
       "544               0.06845          0.2249                  0.08492        1  \n",
       "545               0.07174          0.2642                  0.06953        1  \n",
       "546               0.02381          0.2681                  0.07399        1  \n",
       "547               0.08333          0.2691                  0.09479        1  \n",
       "548               0.03846          0.2552                  0.07920        1  \n",
       "549               0.03264          0.3059                  0.07626        1  \n",
       "550               0.00000          0.2458                  0.06592        1  \n",
       "551               0.06413          0.3169                  0.08032        1  \n",
       "552               0.06498          0.2407                  0.06484        1  \n",
       "553               0.02564          0.2435                  0.07393        1  \n",
       "554               0.06493          0.2372                  0.07242        1  \n",
       "555               0.09127          0.2226                  0.08283        1  \n",
       "556               0.02232          0.2262                  0.06742        1  \n",
       "557               0.00000          0.2475                  0.06969        1  \n",
       "558               0.11050          0.2258                  0.08004        1  \n",
       "559               0.09653          0.2112                  0.08732        1  \n",
       "560               0.10480          0.2250                  0.08321        1  \n",
       "561               0.00000          0.1566                  0.05905        1  \n",
       "562               0.23560          0.4089                  0.14090        0  \n",
       "563               0.25420          0.2929                  0.09873        0  \n",
       "564               0.22160          0.2060                  0.07115        0  \n",
       "565               0.16280          0.2572                  0.06637        0  \n",
       "566               0.14180          0.2218                  0.07820        0  \n",
       "567               0.26500          0.4087                  0.12400        0  \n",
       "568               0.00000          0.2871                  0.07039        1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the dataset from sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Convert to Pandas dataframe for some exploration\n",
    "data_pd = pd.DataFrame(columns=data.feature_names, data=data.data)\n",
    "data_pd['outcome'] = data.target\n",
    "\n",
    "data_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Machine Learning Model: A Decision Tree\n",
    "Without doing any pre-processing (or considering the idea of training/testing data), let's try implementing our first machine learning model, a **Decision Tree**. Like any model, it will involve a few steps (to be elaborated upon as we go):\n",
    "\n",
    "- **Import** your model from `sklearn`\n",
    "- **Create** your model with the desired parameters\n",
    "- **Fit** your model to the data\n",
    "- **Assess** performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your model, a decision tree classifier (typically imported at the beginning of your script)\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=11,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a classifier, then fit the model to your data (creating a decision tree)\n",
    "# To ensure consistent results, we can use the random_state parameter\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=11)\n",
    "tree_fit = tree_clf.fit(data.data, data.target)\n",
    "tree_fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assess the fit of our data by generating predictions using your data\n",
    "# Then calculate the accuracy (percentage of the time that the predictions equal the data)\n",
    "\n",
    "tree_preds = tree_fit.predict(data.data)\n",
    "(tree_preds == data.target).sum()/len(data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Data\n",
    "In actual machine learning applications, the intention is to **predict unknown values**. In the example above, you were able to perfectly predict the values because _your model has seen them_. To simulate a _real world_ application, it's necessary to **hide testing data from your model**, and then use your hidden (**test**) data to assess your performance. This can easily be achieved using the `test_train_split` method from `sklearn`. We'll augment our machine learning process to now include splitting our dataset into testing and training data:\n",
    "\n",
    "\n",
    "\n",
    "- **Import** your model\n",
    "- **Create** your model\n",
    "- <span style=\"color:red\">**Split** data into training and testing data</span>\n",
    "- **Fit** your model to **training** data\n",
    "- **Assess** performance of the model on your **test** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data into test and training data with a test size of 30% (.3)\n",
    "from sklearn.model_selection import train_test_split # typically done at the start of the script\n",
    "train_features, test_feature, train_outcome, test_outcome = train_test_split(data.data, \n",
    "                                                                             data.target, \n",
    "                                                                             test_size = 0.3,\n",
    "                                                                             random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier -- from above -- using our (training) data \n",
    "tree_fit = tree_clf.fit(train_features, train_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assess the model using our (test) data. \n",
    "# You can import and use the `accuracy_score` fucntion rather than manually computing it\n",
    "from sklearn.metrics import accuracy_score\n",
    "tree_preds = tree_fit.predict(test_feature)\n",
    "accuracy_score(tree_preds, test_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not required: use graphviz to visualize your tree\n",
    "#import graphviz\n",
    "from sklearn import tree \n",
    "tree.export_graphviz(tree_clf, feature_names=data.feature_names, class_names=['Benign', 'Malig.'], out_file=\"mytree.dot\")\n",
    "# To conver, you'll need to install graphviz: conda install grahpviz\n",
    "# Then on your terminal: dot -Tpng mytree.dot -o mytree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Second Model: K-Nearest Neighbors\n",
    "K-Nearest Neighbors is a very simple algorithm. To identify the classification of any observation (row), we simply need to look at the class of K similar points. This introduces two new questions:\n",
    "\n",
    "1. How do we decide on the number of neighbors to use for classifying any given point (K)?\n",
    "2. How do we calculate distance when our features (columns) are in different units?\n",
    "\n",
    "We'll address each of these challenges in the following sections. However, we'll start by simply implementing the algorithm (which will follow the same process as other algorithms). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and create a KNN classifier that uses the 4 nearest points\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to our (training) data, and use it to predict on our *test data*\n",
    "knn_preds = knn_clf.fit(train_features, train_outcome).predict(test_feature)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9181286549707602"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assess the accuracy of the model\n",
    "accuracy_score(knn_preds, test_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking the number of neighbors (K)\n",
    "In the above section, we arbitrarily used the number **4** for K. We want to use a data driven process for picking this value. Because we **don't look at the test data** until the end, we need another strategy for assessing model parameters. To do this, we can further split the training data into a **validation set** (think of this as a mini-test set), which we can use to search for the best value of **K**. Our process now involves:\n",
    "\n",
    "\n",
    "- **Import** your model\n",
    "- **Create** your model\n",
    "- **Split** data into training and testing data\n",
    "- <span style=\"color:red\">**Split** _training_ data into (smaller) training and **validation** data</span>\n",
    "- **Fit** your model to (smaller) **training** data\n",
    "- <span style=\"color:red\">**Assess** performance of the model on your **validation** data to tweak parameters</span>\n",
    "- **Assess** performance of the model on your **test** data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's crete a validation set by sampling 20% (.2) out of the *training* data\n",
    "# We can use the `train_test_set` function as before\n",
    "train_features_small, validation_features, train_outcome_small, validation_outcome = train_test_split(\n",
    "                                                                                     train_features,\n",
    "                                                                                     train_outcome,                      \n",
    "                                                                                     test_size = 0.2,\n",
    "                                                                                     random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.9,\n",
       " 2: 0.925,\n",
       " 3: 0.9125,\n",
       " 4: 0.925,\n",
       " 5: 0.925,\n",
       " 6: 0.9375,\n",
       " 7: 0.9375,\n",
       " 8: 0.925,\n",
       " 9: 0.925,\n",
       " 10: 0.9375,\n",
       " 15: 0.925,\n",
       " 100: 0.875}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's assess accuracies of K from 1 through 10. To do this, loop through values of K, \n",
    "# and in each loop:\n",
    "# - Create a new classifier using K as the number of neighbors, \n",
    "# - Fit the classifier to the (small) training data (without validation data)\n",
    "# - Generate a set of predictions using the validation data\n",
    "# - Compute the accuracy of your model on your validation data\n",
    "accuracies = {}\n",
    "for k in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 100]:\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn_preds = knn_clf.fit(train_features_small, train_outcome_small).predict(validation_features)\n",
    "    accuracies[k] = accuracy_score(knn_preds, validation_outcome)\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298245614035088"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that you know the best value of K based on your *validation* data, \n",
    "# Run the KNN with the optimal value of K on the *test* data to assess your performance\n",
    "best_knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "knn_preds = best_knn.fit(train_features, train_outcome).predict(test_feature)\n",
    "accuracy_score(knn_preds, test_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "In the above example, we created a **single** validation dataset from our training data. However, because this sampling is **random**, we may have gotten a certain subset of the training set simply **by chance**. The most popular strategy for solving this issue (and therefore better leveraging the information in our _training data_) is called **cross validation**. \n",
    "\n",
    "There are a number of strategies for cross-validation, but the philosophy is the same: split the training data into a (smaller) training and a validation set **multiple times**. For example, a popular approach is the **K Fold** cross validation which splits the data into **K** folds (_not_ to be confused with the **K** from **KNN**). Then, each _fold_ of the data is used as the validation set one time. This allows each observation to appear in the validation set one time. You then assess model performance for each fold, and pick your best model based on the **average** performance across the folds. \n",
    "\n",
    "Luckily, the `sklearn` toolkit has a variety of built-in methods for performing this process. This is really only a minor change to our process:\n",
    "\n",
    "- **Import** your model\n",
    "- **Create** your model\n",
    "- **Split** data into training and testing data\n",
    "- <span style=\"text-decoration:line-through\">**Split** _training_ data into (smaller) training and **validation** data</span> (accomplished in new step below)\n",
    "- <span style=\"color:red\">**Fit** and **assess** your model using `cross_val_score` to fit the model on different _folds_ of validation data</span>\n",
    "- <span style=\"text-decoration:line-through\">**Assess** performance of the model on your **validation** data to tweak parameters</span> (accomplished in new step above)\n",
    "- **Assess** performance of the model on your **test** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9221794871794872"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use cross validation (`cross_val_score`) to test a classifier across 10 different splits of the data\n",
    "# Use a K value of 3 for your KNN.\n",
    "# Notice the huge variation in performance across folds!\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3)\n",
    "folds = KFold(n_splits = 10, shuffle = True)\n",
    "np.mean(cross_val_score(knn_clf, train_features, train_outcome, cv = folds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "Unfortunately, the above code only runs a **single model** across **K** different _folds_ of the data. What we really want to do is iterate over our parameter space (the number of different **neighbors**) for each of the folds. While this would be simple enough to write as a loop, we can easily use the **grid search** functionality built into the `sklearn` toolkit. The benefit is that grid search will seach over a _grid_ of **all parameters** of interest regarding your model (while we only have one current parameter to optimize, we could have many). This will allow you to quickly identify the optimal set of parameters for your model. \n",
    "\n",
    "\n",
    "- **Import** your model\n",
    "- **Create** your model\n",
    "- **Split** data into training and testing data\n",
    "- <span style=\"text-decoration:line-through\">**Fit** and **assess** your model using `cross_val_score` to fit the model on different _folds_ of validation data</span>\n",
    "- <span style=\"color:red\">**Fit** and **assess** models using `grid_search` to fit multiple models using **different parameters** on different _folds_ of validation data</span>\n",
    "- <span style=\"text-decoration:line-through\">**Assess** performance of the model on your **validation** data to tweak parameters</span> (accomplished in new step above)\n",
    "- **Assess** performance of the (best) model on your **test** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': range(1, 50), 'weights': ['uniform', 'distance']}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the `GridSearchCV` model, and specify a grid of parameters\n",
    "# This will try all possible combination of parameters in a brute force way\n",
    "# And, it will run cross validation on each combination of parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\"n_neighbors\":range(1,50), \"weights\":[\"uniform\", \"distance\"]}\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': range(1, 50), 'weights': ['uniform', 'distance']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a grid search estimator for your KNN classifier\n",
    "# When fit, this will search the parameter grid using cross validation\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid = params, cv=folds, return_train_score=True)\n",
    "grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': range(1, 50), 'weights': ['uniform', 'distance']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the grid search to your training data\n",
    "grid_search.fit(train_features, train_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298245614035088"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict and assess performance on your **test** data\n",
    "grid_search.score(test_feature, test_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a14a877f0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualze performance across neighbors N values\n",
    "grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "uniform_results = grid_search.cv_results_['mean_test_score'][grid_search.cv_results_['param_weights'].data == \"uniform\"]\n",
    "distance_results = grid_search.cv_results_['mean_test_score'][grid_search.cv_results_['param_weights'].data == \"distance\"]\n",
    "plt.plot(range(1,50), uniform_results, label=\"uniform\")\n",
    "plt.plot(range(1,50), distance_results, label=\"distance\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Data\n",
    "Because KNN uses a distance based metric to compute similarity, it's important to **normalize** each column to the same scale before running the algorithm. Note, some rule based estimators (like decision trees) aren't sensitive to feature scale (and don't need to be normalized). For example, we can use a `MinMaxScaler` that subracts the minimum, and divides by the difference between the minimum and the maximum of each column.\n",
    "\n",
    "In order to not **leak information** from out test data into our training data, it's important to normalize our data **after** splitting. We'll worry about integrating this into our process after experimenting with it in the section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually scale (pre-process) our *training features* (not the outcome!) data\n",
    "train_features, test_features, train_outcome, test_outcome = train_test_split(data.data,data.target,test_size=0.30)\n",
    "train_features_scaled = (train_features - train_features.min(axis=0)) / (train_features.max(axis=0) - train_features.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we could use a built in Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled_alt = scaler.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a knn classifier with the scaled training data\n",
    "knn_clf.fit(train_features_scaled, train_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions using *scaled test data*\n",
    "test_scaled = scaler.fit_transform(test_features)\n",
    "accuracy_score(knn_clf.predict(test_scaled), test_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Pipelines\n",
    "Our final challenge is determining how to integrate the data scaling into our cross validation process. For **each set** of possible model parameters, we need to:\n",
    "\n",
    "- **Split** our (training) data into training and validation sets\n",
    "- **Normalize**/scale each training set\n",
    "- **Assess** the model on all validation sets\n",
    "\n",
    "\n",
    "While we could write the iterators to do this ourselves, this is a problem that is already solved using **Pipelines**. The `GridSearchCV` class is structured to take as an argument a _pipeline_ object, which includes each transformation or calculation you want to perform on your data. For example\n",
    "\n",
    "```python\n",
    "# Define scaler, classifier, and pipeline to use\n",
    "scaler = MinMaxScaler()\n",
    "knn_clf = KNeighborsClassifier()\n",
    "pipe = make_pipeline(scaler, knn_clf)\n",
    "\n",
    "# Define grid for pipeline (indicates which arguments are for which classes)\n",
    "param_grid = {'kneighborsclassifier__n_neighbors': [1, 3, 5, 10]}\n",
    "\n",
    "# Search through and perform cross validation\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "```\n",
    "\n",
    "Our final process will be:\n",
    "\n",
    "- **Import** your model\n",
    "- **Create** your model\n",
    "- **Split** data into training and testing data\n",
    "- **Create** your normalization (scaling) function\n",
    "- **Define** a **pipeline** that will implement your pre-processing and your model\n",
    "- **Fit** and **assess** models using `grid_search` to fit multiple models using **different parameters** on different _folds_ of validation data</span>\n",
    "- **Assess** performance of the (best) model on your **test** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline, and define your pipeline that will\n",
    "# - Transform you data using a MinMaxScaler\n",
    "# - Fit a KNN classifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(MinMaxScaler(), KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass your pipeline to a grid search, specifying a set of neighbors to assess\n",
    "params = {\"kneighborsclassifier__n_neighbors\":range(1,50), \"kneighborsclassifier__weights\":[\"uniform\", \"distance\"]}\n",
    "grid_search = GridSearchCV(pipe, params, cv=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('kneighborsclassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'kneighborsclassifier__n_neighbors': range(1, 50), 'kneighborsclassifier__weights': ['uniform', 'distance']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(train_features, train_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707602339181286"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.score(test_features, test_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
